"""
Standard sequential workflow for JPEG2000 processing.

This module provides a sequential implementation of the JPEG2000 workflow.
"""

import os
import gc
import time
import logging
import tempfile
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple

from core.types import (
    WorkflowStatus,
    DocumentType,
    CompressionMode,
    ProcessingResult,
    WorkflowConfig,
    BnFCompressionRatio
)
from core.metadata.base_handler import MetadataHandler
from core.metadata.bnf_handler import BnFMetadataHandler
from utils.image import (
    validate_image, get_output_path, find_image_files,
    is_multipage_tiff, extract_tiff_page, get_multipage_output_paths
)
from utils.profiling import profile, profile_block, mark_event, save_report
from workflow.base import BaseWorkflow

logger = logging.getLogger(__name__)


class StandardWorkflow(BaseWorkflow):
    """Sequential implementation of the JPEG2000 workflow."""

    def __init__(self, config: WorkflowConfig):
        """Initialize the workflow with configuration.

        Args:
            config: Workflow configuration
        """
        super().__init__(config)

        # Initialize performance profiling for the workflow
        self.enable_profiling = config.enable_profiling if hasattr(
            config, 'enable_profiling') else False
        if self.enable_profiling:
            from utils.profiling import get_profiler
            get_profiler(output_dir=config.report_dir)
            logger.info("Performance profiling enabled for StandardWorkflow")

        # Create temporary directory for extracted TIFF pages
        self.temp_dir = tempfile.mkdtemp(prefix="jp2forge_")

    def __del__(self):
        """Clean up temporary files when workflow is destroyed."""
        self._cleanup_temp_files()

    def _cleanup_temp_files(self):
        """Remove temporary files created for multi-page TIFF processing."""
        try:
            import shutil
            if hasattr(self, 'temp_dir') and os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                logger.debug(f"Removed temporary directory: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"Failed to clean up temporary files: {str(e)}")

    @profile("process_file_implementation")
    def _process_file_implementation(
        self,
        input_file: str,
        doc_type: DocumentType,
        lossless_fallback: bool,
        bnf_compliant: bool,
        compression_ratio_tolerance: float,
        include_bnf_markers: bool,
        metadata: Dict[str, Any]
    ) -> ProcessingResult:
        """Implementation of file processing logic.

        Args:
            input_file: Path to input file
            doc_type: Document type for compression
            lossless_fallback: Whether to fall back to lossless compression
            bnf_compliant: Whether to use BnF compliant settings
            compression_ratio_tolerance: Tolerance for compression ratio
            include_bnf_markers: Whether to include BnF robustness markers
            metadata: Additional metadata to include in output file

        Returns:
            ProcessingResult: Result of processing
        """
        # Check if input is a multi-page TIFF
        is_multipage, page_count = is_multipage_tiff(input_file)

        if is_multipage:
            # Handle multi-page TIFF
            return self._process_multipage_tiff(
                input_file=input_file,
                page_count=page_count,
                doc_type=doc_type,
                lossless_fallback=lossless_fallback,
                bnf_compliant=bnf_compliant,
                compression_ratio_tolerance=compression_ratio_tolerance,
                include_bnf_markers=include_bnf_markers,
                metadata=metadata
            )

        # Original code for single page images
        try:
            # Step 1: Convert to JPEG2000
            with profile_block("conversion_to_jp2"):
                logger.info("Step 1: Converting to JPEG2000")
                output_file = get_output_path(input_file, self.config.output_dir, ".jp2")

                processing_start = time.time()

                # Process according to compression mode
                compression_mode = self.config.compression_mode
                success = self.compressor.convert_to_jp2(
                    input_file,
                    output_file,
                    doc_type,
                    compression_mode,
                    lossless_fallback,
                    bnf_compliant,
                    compression_ratio_tolerance,
                    include_bnf_markers
                )

                processing_time = time.time() - processing_start
                logger.info(f"Conversion completed in {processing_time:.2f} seconds")

                if not success:
                    mark_event("conversion_failed", {"input_file": input_file})
                    return ProcessingResult(
                        status=WorkflowStatus.FAILURE,
                        input_file=input_file,
                        error="Conversion failed"
                    )

            # Step 2: Analyze pixel loss (only in supervised mode)
            report_file = None
            analysis_results = None

            if compression_mode == CompressionMode.SUPERVISED:
                with profile_block("pixel_loss_analysis"):
                    logger.info("Step 2: Analyzing pixel loss")
                    analysis_start = time.time()

                    analysis_result = self.analyzer.analyze_pixel_loss(
                        input_file,
                        output_file,
                        save_report=False  # Changed from True to False to avoid generating individual analysis files
                    )

                    analysis_time = time.time() - analysis_start
                    logger.info(f"Analysis completed in {analysis_time:.2f} seconds")

                    # Check if quality checks failed but we're still returning success
                    if not analysis_result.quality_passed:
                        logger.warning(f"Quality checks failed for {input_file}")
                        mark_event("quality_check_failed", {
                            "input_file": input_file,
                            "psnr": analysis_result.psnr,
                            "ssim": analysis_result.ssim,
                            "mse": analysis_result.mse
                        })
                        # We still return success but with metrics for user review
                        status = WorkflowStatus.WARNING
                        metrics = {
                            "psnr": analysis_result.psnr,
                            "ssim": analysis_result.ssim,
                            "mse": analysis_result.mse,
                            "quality_passed": analysis_result.quality_passed
                        }
                    else:
                        status = WorkflowStatus.SUCCESS
                        metrics = {
                            "psnr": analysis_result.psnr,
                            "ssim": analysis_result.ssim,
                            "mse": analysis_result.mse,
                            "quality_passed": analysis_result.quality_passed
                        }
            else:
                # For non-supervised modes, we don't do analysis
                status = WorkflowStatus.SUCCESS
                metrics = None

            # Step 3: Add metadata
            with profile_block("add_metadata"):
                logger.info("Step 3: Adding metadata")
                # Pass compression parameters to metadata handler
                compression_mode_str = self.config.compression_mode.value

                # For BnF mode, use BnF compliant metadata
                if bnf_compliant or compression_mode == CompressionMode.BNF_COMPLIANT:
                    # Use the BnF metadata handler
                    # Initialize with base handler to ensure proper setup
                    if not isinstance(self.metadata_handler, BnFMetadataHandler):
                        base_handler = MetadataHandler()
                        bnf_handler = BnFMetadataHandler(base_handler=base_handler, debug=True)
                    else:
                        bnf_handler = self.metadata_handler

                    # Generate document ID if not provided in metadata
                    metadata_dict = {}
                    if metadata:
                        metadata_dict.update(metadata)

                    # If not provided, generate default BnF isPartOf ID (NUM_format)
                    if 'dcterms:isPartOf' not in metadata_dict:
                        base_name = os.path.splitext(os.path.basename(input_file))[0]
                        metadata_dict['dcterms:isPartOf'] = f"NUM_{base_name}"

                    # Default BnF provenance if not specified
                    if 'dcterms:provenance' not in metadata_dict:
                        metadata_dict['dcterms:provenance'] = "Biblioth√®que nationale de France"

                    try:
                        self.metadata_handler.write_metadata(
                            output_file,
                            metadata_dict,
                            compression_mode_str,
                            self.config.num_resolutions,
                            self.config.progression_order,
                            True  # bnf_compliant=True
                        )
                    except Exception as e:
                        mark_event("metadata_error", {"error": str(e), "type": "bnf_metadata"})
                        logger.error(f"Error writing BnF metadata: {str(e)}")
                        return ProcessingResult(
                            status=WorkflowStatus.WARNING,  # Continue with warning
                            input_file=input_file,
                            output_file=output_file,
                            error=f"Converted successfully but metadata failed: {str(e)}"
                        )
                else:
                    # Standard metadata
                    try:
                        self.metadata_handler.write_metadata(
                            output_file,
                            metadata,
                            compression_mode_str,
                            self.config.num_resolutions,
                            self.config.progression_order
                        )
                    except Exception as e:
                        mark_event("metadata_error", {"error": str(e), "type": "standard_metadata"})
                        logger.error(f"Error writing metadata: {str(e)}")
                        return ProcessingResult(
                            status=WorkflowStatus.WARNING,  # Continue with warning
                            input_file=input_file,
                            output_file=output_file,
                            error=f"Converted successfully but metadata failed: {str(e)}"
                        )

            # Force garbage collection
            with profile_block("cleanup"):
                gc.collect()

            # Calculate file sizes
            file_sizes = None
            try:
                original_size = os.path.getsize(input_file)
                converted_size = os.path.getsize(output_file)
                compression_ratio = original_size / converted_size if converted_size > 0 else 0

                file_sizes = {
                    "original_size": original_size,
                    "original_size_human": self._format_file_size(original_size),
                    "converted_size": converted_size,
                    "converted_size_human": self._format_file_size(converted_size),
                    "compression_ratio": f"{compression_ratio:.2f}:1"
                }

                # Log compression stats for profiling
                if self.enable_profiling:
                    mark_event("compression_stats", {
                        "input_file": os.path.basename(input_file),
                        "original_size_mb": original_size / (1024 * 1024),
                        "converted_size_mb": converted_size / (1024 * 1024),
                        "compression_ratio": compression_ratio
                    })
            except Exception as e:
                logger.warning(f"Error calculating file sizes: {str(e)}")

            logger.info(f"Successfully processed {input_file}")
            self.processed_files_count += 1

            return ProcessingResult(
                status=status,
                input_file=input_file,
                output_file=output_file,
                report_file=report_file,
                metrics=metrics,
                file_sizes=file_sizes
            )

        except Exception as e:
            logger.error(f"Error processing {input_file}: {str(e)}")
            mark_event("processing_error", {"input_file": input_file, "error": str(e)})
            return ProcessingResult(
                status=WorkflowStatus.FAILURE,
                input_file=input_file,
                error=str(e)
            )

    def _process_multipage_tiff(
        self,
        input_file: str,
        page_count: int,
        doc_type: DocumentType,
        lossless_fallback: bool,
        bnf_compliant: bool,
        compression_ratio_tolerance: float,
        include_bnf_markers: bool,
        metadata: Dict[str, Any]
    ) -> ProcessingResult:
        """
        Process a multi-page TIFF file by extracting and converting each page.

        Args:
            input_file: Path to input file
            page_count: Number of pages in the TIFF file
            doc_type: Document type for compression
            lossless_fallback: Whether to fall back to lossless compression
            bnf_compliant: Whether to use BnF compliant settings
            compression_ratio_tolerance: Tolerance for compression ratio
            include_bnf_markers: Whether to include BnF robustness markers
            metadata: Additional metadata to include in output file

        Returns:
            ProcessingResult: Result of processing, combining results for all pages
        """
        logger.info(f"Processing multi-page TIFF: {input_file} with {page_count} pages")

        # Generate output paths for all pages
        output_paths = get_multipage_output_paths(
            input_file,
            self.config.output_dir,
            ".jp2",
            page_count
        )

        # If the "skip_existing" flag is set, check if output files exist and skip them
        if hasattr(self.config, 'skip_existing') and self.config.skip_existing:
            existing_files = [p for p in output_paths if os.path.exists(p)]
            if existing_files and len(existing_files) == len(output_paths):
                logger.info(f"Skipping {input_file}: All output files already exist")
                return ProcessingResult(
                    status=WorkflowStatus.SUCCESS,
                    input_file=input_file,
                    output_file=",".join(existing_files),
                    error=None,
                    file_sizes=None,
                    metrics={"skipped": True, "reason": "All output files already exist"}
                )

        # Check if we should overwrite existing files
        overwrite = hasattr(self.config, 'overwrite') and self.config.overwrite
        if not overwrite:
            # Filter out paths that already exist and warn
            for path in output_paths:
                if os.path.exists(path):
                    logger.warning(
                        f"Output file already exists and will not be overwritten: {path}")

        # Track results for all pages
        page_results = []
        overall_status = WorkflowStatus.SUCCESS
        combined_file_sizes = {
            "original_size": 0,
            "converted_size": 0,
            "pages": []
        }

        # Apply chunk size and memory limit settings
        chunk_size = self.config.chunk_size if hasattr(self.config, 'chunk_size') else 1000000
        memory_limit_mb = self.config.memory_limit_mb if hasattr(
            self.config, 'memory_limit_mb') else 4096

        # Check if we need memory-efficient processing
        # Using chunk_size and memory_limit_mb instead of the previously used force_chunking
        use_memory_efficient_processing = (hasattr(self.config, 'chunk_size') and
                                           self.config.chunk_size < 1000000) or \
            (hasattr(self.config, 'memory_limit_mb') and
             self.config.memory_limit_mb < 2048)

        # Process each page
        for page_num in range(page_count):
            logger.info(f"Processing page {page_num+1}/{page_count} from {input_file}")
            output_file = output_paths[page_num]

            # Skip existing files if not overwriting
            if not overwrite and os.path.exists(output_file):
                logger.info(f"Skipping existing output file: {output_file}")
                try:
                    page_original_size = 0  # Unknown since we're skipping
                    page_converted_size = os.path.getsize(output_file)

                    # Add to combined sizes
                    combined_file_sizes["converted_size"] += page_converted_size

                    # Add page-specific info
                    combined_file_sizes["pages"].append({
                        "page": page_num + 1,
                        "original_size": 0,  # Unknown
                        "original_size_human": "N/A",  # Unknown
                        "converted_size": page_converted_size,
                        "converted_size_human": self._format_file_size(page_converted_size),
                        "compression_ratio": "N/A"  # Unknown
                    })

                    page_results.append({
                        "page": page_num + 1,
                        "status": "SUCCESS",
                        "output_file": output_file,
                        "skipped": True
                    })

                    continue
                except Exception as e:
                    logger.warning(f"Error checking existing file {output_file}: {str(e)}")

            try:
                # Extract the page to a temporary file
                temp_tiff = extract_tiff_page(input_file, page_num, self.temp_dir)

                # Process the extracted page (similar to single page processing)
                with profile_block(f"page{page_num}_conversion"):
                    # Add page number to metadata
                    page_metadata = metadata.copy() if metadata else {}
                    page_metadata["tiff:PageNumber"] = f"{page_num+1}/{page_count}"

                    # Process according to compression mode
                    compression_mode = self.config.compression_mode

                    # Set any additional compressor options specific to this workflow config
                    if hasattr(self.config, 'use_memory_pool') and hasattr(self.compressor, 'use_memory_pool'):
                        self.compressor.use_memory_pool = self.config.use_memory_pool
                    if hasattr(self.config, 'memory_pool_size_mb') and hasattr(self.compressor, 'memory_pool_size_mb'):
                        self.compressor.memory_pool_size_mb = self.config.memory_pool_size_mb

                    # Use custom target compression ratio if specified
                    if hasattr(self.config, 'target_compression_ratio') and self.config.target_compression_ratio:
                        logger.info(
                            f"Using target compression ratio: {self.config.target_compression_ratio}")
                        # Since target_compression_ratio is not a standard parameter for convert_to_jp2,
                        # we'll need to handle this with the specific encoding implementation in the future

                    # Configure memory-efficient processing if needed
                    if use_memory_efficient_processing:
                        logger.info(
                            f"Using memory-efficient processing for page {page_num+1} with chunk_size={chunk_size}, memory_limit_mb={memory_limit_mb}")
                        # We'll need to implement this in the compressor

                    # Convert using all specified parameters
                    success = self.compressor.convert_to_jp2(
                        temp_tiff,
                        output_file,
                        doc_type,
                        compression_mode,
                        lossless_fallback,
                        bnf_compliant,
                        compression_ratio_tolerance,
                        include_bnf_markers
                    )

                    if not success:
                        logger.error(f"Failed to convert page {page_num+1} from {input_file}")
                        overall_status = WorkflowStatus.FAILURE
                        page_results.append({
                            "page": page_num + 1,
                            "status": "FAILURE",
                            "output_file": None,
                            "error": "Conversion failed"
                        })
                        continue

                # Add metadata for this page
                try:
                    # Choose the right metadata handler based on BnF compliance
                    if bnf_compliant or compression_mode == CompressionMode.BNF_COMPLIANT:
                        # Use the BnF metadata handler
                        if not isinstance(self.metadata_handler, BnFMetadataHandler):
                            base_handler = MetadataHandler()
                            bnf_handler = BnFMetadataHandler(base_handler=base_handler, debug=True)
                        else:
                            bnf_handler = self.metadata_handler

                        # Generate document ID if not provided in metadata
                        if 'dcterms:isPartOf' not in page_metadata:
                            base_name = os.path.splitext(os.path.basename(input_file))[0]
                            page_metadata['dcterms:isPartOf'] = f"NUM_{base_name}_p{page_num+1}"

                        # Default BnF provenance if not specified
                        if 'dcterms:provenance' not in page_metadata:
                            page_metadata['dcterms:provenance'] = "Biblioth√®que nationale de France"

                        self.metadata_handler.write_metadata(
                            output_file,
                            page_metadata,
                            compression_mode.value,
                            self.config.num_resolutions,
                            self.config.progression_order,
                            True  # bnf_compliant=True
                        )
                    else:
                        # Standard metadata
                        self.metadata_handler.write_metadata(
                            output_file,
                            page_metadata,
                            compression_mode.value,
                            self.config.num_resolutions,
                            self.config.progression_order
                        )
                except Exception as e:
                    logger.warning(f"Failed to write metadata for page {page_num+1}: {str(e)}")
                    # Continue processing despite metadata error

                # Run analysis if in supervised mode
                if compression_mode == CompressionMode.SUPERVISED:
                    try:
                        analysis_result = self.analyzer.analyze_pixel_loss(
                            temp_tiff,
                            output_file,
                            save_report=False
                        )

                        # Add quality metrics to page results
                        page_metrics = {
                            "psnr": analysis_result.psnr,
                            "ssim": analysis_result.ssim,
                            "mse": analysis_result.mse,
                            "quality_passed": analysis_result.quality_passed
                        }
                    except Exception as e:
                        logger.warning(f"Error analyzing quality for page {page_num+1}: {str(e)}")
                        page_metrics = None
                else:
                    page_metrics = None

                # Calculate file sizes for this page
                try:
                    page_original_size = os.path.getsize(temp_tiff)
                    page_converted_size = os.path.getsize(output_file)
                    page_ratio = page_original_size / page_converted_size if page_converted_size > 0 else 0

                    # Add to combined sizes
                    combined_file_sizes["original_size"] += page_original_size
                    combined_file_sizes["converted_size"] += page_converted_size

                    # Add page-specific info
                    combined_file_sizes["pages"].append({
                        "page": page_num + 1,
                        "original_size": page_original_size,
                        "original_size_human": self._format_file_size(page_original_size),
                        "converted_size": page_converted_size,
                        "converted_size_human": self._format_file_size(page_converted_size),
                        "compression_ratio": f"{page_ratio:.2f}:1"
                    })

                    # Log compression stats for profiling
                    if self.enable_profiling:
                        mark_event("compression_stats", {
                            "input_file": f"{os.path.basename(input_file)}_page_{page_num+1}",
                            "original_size_mb": page_original_size / (1024 * 1024),
                            "converted_size_mb": page_converted_size / (1024 * 1024),
                            "compression_ratio": page_ratio
                        })

                except Exception as e:
                    logger.warning(f"Error calculating file sizes for page {page_num+1}: {str(e)}")

                # Store page result with any metrics
                page_results.append({
                    "page": page_num + 1,
                    "status": "SUCCESS",
                    "output_file": output_file,
                    "metrics": page_metrics
                })

                logger.info(
                    f"Successfully processed page {page_num+1}/{page_count} from {input_file}")

            except Exception as e:
                logger.error(f"Error processing page {page_num+1} from {input_file}: {str(e)}")
                overall_status = WorkflowStatus.FAILURE
                page_results.append({
                    "page": page_num + 1,
                    "status": "FAILURE",
                    "error": str(e)
                })

            # Force garbage collection after each page
            gc.collect()

            # Don't clean up temporary files if keep_temp is enabled
            if hasattr(self.config, 'keep_temp') and self.config.keep_temp:
                logger.info(f"Keeping temporary file for page {page_num+1}: {temp_tiff}")
            else:
                try:
                    if os.path.exists(temp_tiff):
                        os.remove(temp_tiff)
                except Exception as e:
                    logger.warning(f"Failed to remove temporary file {temp_tiff}: {str(e)}")

        # Format combined file sizes
        if combined_file_sizes["original_size"] > 0 and combined_file_sizes["converted_size"] > 0:
            total_ratio = combined_file_sizes["original_size"] / \
                combined_file_sizes["converted_size"]
            combined_file_sizes["original_size_human"] = self._format_file_size(
                combined_file_sizes["original_size"])
            combined_file_sizes["converted_size_human"] = self._format_file_size(
                combined_file_sizes["converted_size"])
            combined_file_sizes["compression_ratio"] = f"{total_ratio:.2f}:1"

        # Check if all pages were processed successfully
        success_count = sum(1 for r in page_results if r["status"] == "SUCCESS")

        if success_count == 0:
            overall_status = WorkflowStatus.FAILURE
            error_message = "Failed to process any pages from multi-page TIFF"
        elif success_count < page_count:
            overall_status = WorkflowStatus.WARNING
            error_message = f"Processed {success_count} of {page_count} pages successfully"
        else:
            overall_status = WorkflowStatus.SUCCESS
            error_message = None

        logger.info(
            f"Completed processing multi-page TIFF: {input_file}, status: {overall_status.name}")

        # Return combined result
        return ProcessingResult(
            status=overall_status,
            input_file=input_file,
            output_file=",".join([r["output_file"] for r in page_results if r.get("output_file")]),
            error=error_message,
            file_sizes=combined_file_sizes,
            metrics={"multipage_results": page_results}
        )

    @profile("process_directory")
    def process_directory(
        self,
        input_dir: str,
        doc_type: Optional[DocumentType] = None,
        metadata: Optional[Dict[str, Any]] = None,
        recursive: Optional[bool] = None,
        lossless_fallback: Optional[bool] = None,
        bnf_compliant: Optional[bool] = None,
        compression_ratio_tolerance: Optional[float] = None,
        include_bnf_markers: Optional[bool] = None
    ) -> Dict[str, Any]:
        """Process all files in a directory sequentially.

        Args:
            input_dir: Directory containing input files
            doc_type: Document type for compression
            metadata: Metadata to add to files
            recursive: Whether to process subdirectories
            lossless_fallback: Whether to fall back to lossless compression
            bnf_compliant: Whether to use BnF compliant settings
            compression_ratio_tolerance: Tolerance for compression ratio
            include_bnf_markers: Whether to include BnF robustness markers

        Returns:
            Dictionary with processing results
        """
        # Validate output directory before processing (implementing BaseWorkflow validation)
        if not self.config.output_dir or not os.path.exists(self.config.output_dir):
            logger.error("Output directory not specified or does not exist")
            return {
                'status': WorkflowStatus.FAILURE,
                'error': 'Output directory not specified or does not exist',
                'processed_files': [],
                'success_count': 0,
                'warning_count': 0,
                'error_count': 1,
                'corrupted_count': 0
            }

        # Use configuration defaults if not specified
        doc_type = doc_type or self.config.document_type
        recursive = self.config.recursive if recursive is None else recursive
        lossless_fallback = (
            self.config.lossless_fallback
            if lossless_fallback is None
            else lossless_fallback
        )
        bnf_compliant = (
            self.config.bnf_compliant
            if bnf_compliant is None
            else bnf_compliant
        )
        compression_ratio_tolerance = (
            self.config.compression_ratio_tolerance
            if compression_ratio_tolerance is None
            else compression_ratio_tolerance
        )
        include_bnf_markers = (
            self.config.include_bnf_markers
            if include_bnf_markers is None
            else include_bnf_markers
        )

        # Initialize default metadata if not provided
        if metadata is None:
            metadata = {}

        if not os.path.exists(input_dir):
            logger.error(f"Input directory not found: {input_dir}")
            return {
                'status': WorkflowStatus.FAILURE,
                'error': 'Input directory not found'
            }

        # Initialize results and tracking
        results = {
            'status': WorkflowStatus.SUCCESS,  # Initial status
            'processed_files': [],
            'success_count': 0,
            'warning_count': 0,
            'error_count': 0,
            'corrupted_count': 0,
            'processing_time': 0
        }

        # Initialize tracking variables
        self.processed_files_count = 0
        self.start_time = time.time()

        # Find all image files to process
        with profile_block("find_image_files"):
            image_files = find_image_files(input_dir, recursive)

        self.total_files = len(image_files)
        logger.info(f"Found {self.total_files} image files to process")

        if self.enable_profiling:
            mark_event("batch_processing_start", {
                "total_files": self.total_files,
                "input_dir": input_dir,
                "recursive": recursive
            })

        # Process files sequentially
        for i, input_file in enumerate(image_files):
            result = self.process_file(
                input_file=input_file,
                doc_type=doc_type,
                lossless_fallback=lossless_fallback,
                bnf_compliant=bnf_compliant,
                compression_ratio_tolerance=compression_ratio_tolerance,
                include_bnf_markers=include_bnf_markers,
                metadata=metadata
            )

            # Convert ProcessingResult to dictionary for report
            file_result = {
                'input_file': result.input_file,
                'status': result.status.name,
                'output_file': result.output_file,
                'report_file': result.report_file,
                'error': result.error
            }

            # Add file sizes if available
            if result.file_sizes:
                file_result['file_sizes'] = result.file_sizes

            results['processed_files'].append(file_result)

            if result.status == WorkflowStatus.SUCCESS:
                results['success_count'] += 1
            elif result.status == WorkflowStatus.WARNING:
                results['warning_count'] += 1
                if results['status'] == WorkflowStatus.SUCCESS:
                    results['status'] = WorkflowStatus.WARNING
            elif result.status == WorkflowStatus.FAILURE:
                results['error_count'] += 1
                results['status'] = WorkflowStatus.FAILURE
            elif result.status == WorkflowStatus.SKIPPED:
                results['corrupted_count'] += 1

            # Update progress
            progress = (len(results['processed_files']) / self.total_files) * 100
            logger.info(
                f"Progress: {progress:.1f}% ({len(results['processed_files'])}/{self.total_files})")

            # If there are errors, we still continue processing other files

            # Log batch progress every 5% or 10 files, whichever comes first
            should_log_progress = (i % 10 == 9) or (i > 0 and int(
                progress / 5) > int(((i - 1) / self.total_files * 100) / 5))
            if should_log_progress and self.enable_profiling:
                current_time = time.time()
                elapsed_time = current_time - self.start_time
                files_per_second = (i + 1) / elapsed_time if elapsed_time > 0 else 0
                estimated_remaining = (self.total_files - (i + 1)) / \
                    files_per_second if files_per_second > 0 else 0

                mark_event("batch_progress", {
                    "processed": i + 1,
                    "total": self.total_files,
                    "percent_complete": progress,
                    "files_per_second": files_per_second,
                    "estimated_remaining_seconds": estimated_remaining
                })

        # Calculate total processing time
        results['processing_time'] = time.time() - self.start_time

        # Generate summary report
        with profile_block("generate_summary_report"):
            summary_report = self._generate_summary_report(results)
            summary_file = os.path.join(self.config.report_dir, 'summary_report.md')
            with open(summary_file, 'w') as f:
                f.write(summary_report)

            results['summary_report'] = summary_file

        # Log results
        logger.info(
            f"Processed {len(results['processed_files'])} files in {results['processing_time']:.2f} seconds: "
            f"{results['success_count']} success, "
            f"{results['warning_count']} warning, "
            f"{results['error_count']} error"
        )
        logger.info(f"Directory processing complete. Status: {results['status'].name}")
        logger.info(f"Summary report: {summary_file}")
        logger.info(
            f"Processing rate: {len(results['processed_files']) / results['processing_time']:.2f} files/second")

        if self.enable_profiling:
            mark_event("batch_processing_complete", {
                "total_files": self.total_files,
                "success_count": results['success_count'],
                "warning_count": results['warning_count'],
                "error_count": results['error_count'],
                "corrupted_count": results['corrupted_count'],
                "total_duration": results['processing_time'],
                "files_per_second": len(results['processed_files']) / results['processing_time']
                if results['processing_time'] > 0 else 0
            })

            # Save performance profile report
            profile_report_path = save_report(
                f"profile_report_standard_workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            logger.info(f"Saved performance profile report to {profile_report_path}")

        return results
